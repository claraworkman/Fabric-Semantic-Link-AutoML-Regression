# ğŸšš Delivery Time Prediction POC

**Predict shipment delivery dates using Microsoft Fabric, Semantic Link, AutoML, and Power BI**

---

## ğŸ“‹ Overview

This Proof of Concept (POC) demonstrates a **complete end-to-end machine learning pipeline built entirely on Microsoft Fabric**, using AutoML, MLflow, Semantic Link, Lakehouse tables, and Power BI.

The solution predicts shipment delivery times and surfaces insights on carrier performance, warehouse efficiency, and prediction accuracy. This POC showcases how an organization can **operationalize AI using Fabric's unified analytics platform** without needing multiple disconnected tools.

### ğŸš€ What This POC Delivers

âœ… **End-to-end ML workflow** - Training â†’ Registry â†’ Scoring â†’ BI reporting  
âœ… **High-accuracy prediction model** - AutoML with Random Forest/XGBoost  
âœ… **Delivery time predictions** - For every shipment in your data  
âœ… **Performance dashboards** - Interactive Power BI reports  
âœ… **Fully repeatable pattern** - Scalable, Fabric-native architecture  

### Business Value

- **Proactive customer communication** - Accurate delivery date predictions
- **Resource optimization** - Better planning for warehouse and logistics
- **Identify delays early** - Flag shipments at risk of missing SLAs
- **Data-driven insights** - Understand key factors impacting delivery times
- **Unified platform** - All analytics workloads in one place (Fabric)

---

## ğŸ¯ Why Semantic Link?

**Semantic Link** bridges Power BI semantic models and Python notebooks in Fabric, providing key advantages:

- **Single Source of Truth** - Use your existing Power BI semantic model; no data duplication needed
- **Leverage Existing Work** - Reuse semantic models and relationships already created by your BI team
- **Always Current** - Live data access means you're always working with the latest information
- **Simplified Code** - Pre-joined tables and business logic reduce complexity
- **Full Circle** - Train on BI data â†’ Score predictions â†’ Power BI reports (all in Fabric)

```python
import sempy.fabric as fabric

# Read tables directly from your Power BI semantic model
shipments = fabric.read_table("delivery semantic model", "shipments")
carriers = fabric.read_table("delivery semantic model", "carriers")
warehouses = fabric.read_table("delivery semantic model", "warehouses")
```

---

## ğŸ—ï¸ Architecture

### Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fabric Semantic â”‚
â”‚     Model       â”‚ â—„â”€â”€â”€ Existing shipping data
â”‚  (Shipping Data)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Semantic Link
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fabric Notebookâ”‚
â”‚  Data Prep +    â”‚
â”‚  Feature Eng    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AutoML        â”‚
â”‚   Training      â”‚ â—„â”€â”€â”€ FLAML (LightGBM, XGBoost, RF)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLflow Model   â”‚
â”‚   Registry      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch Scoring   â”‚
â”‚   Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fabric Lakehouseâ”‚
â”‚ (Delta Table)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Power BI      â”‚
â”‚    Report       â”‚ â—„â”€â”€â”€ Direct Lake mode
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

**Step 1: Data Preparation**
- Load shipments, carriers, and warehouses from your Power BI semantic model
- Feature engineering automatically creates prediction variables
- Data validation ensures quality

**Step 2: Model Training**
- AutoML finds the best model (Random Forest, XGBoost, or Extra Trees)
- Model registered in MLflow for tracking and versioning
- Training takes ~3 minutes

**Step 3: Generate Predictions**
- Score all shipments with predicted delivery times
- Write predictions back to Lakehouse
- Predictions appear automatically in Power BI

**Step 4: Power BI Dashboards**
- **Executive Overview** - Model performance and key metrics
- **Carrier Performance** - Identify best/worst carriers
- **Shipment Explorer** - Detailed shipment analysis with filters

---

## ğŸ“‚ Repository Structure

```
delivery-time-prediction-poc/
â”‚
â”œâ”€â”€ 01_semantic_link_data_preparation.ipynb    # Load data from semantic model
â”œâ”€â”€ 02_autoML_training_pipeline.ipynb          # AutoML training + MLflow registry
â”œâ”€â”€ 03_batch_scoring_pipeline.ipynb            # Batch predictions
â”‚
â”œâ”€â”€ notebooks/                                  # Utility modules
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ preprocessing.py                    # Data validation, cleaning, encoding
â”‚       â”œâ”€â”€ feature_engineering.py              # Feature creation
â”‚       â””â”€â”€ model_utils.py                      # Evaluation, MLflow helpers
â”‚
â”œâ”€â”€ powerbi/                                    # Power BI artifacts
â”‚   â”œâ”€â”€ delivery semantic model.pbix            # Semantic model
â”‚   â””â”€â”€ dax/
â”‚       â”œâ”€â”€ measures_basic.dax                  # Basic DAX measures
â”‚       â”œâ”€â”€ measures_advanced.dax               # Advanced metrics
â”‚       â””â”€â”€ README.md                           # DAX documentation
â”‚
â”œâ”€â”€ data/                                       # Data documentation
â”‚   â””â”€â”€ schema/
â”‚       â”œâ”€â”€ shipments_schema.json               # Shipments table schema
â”‚       â”œâ”€â”€ carriers_schema.json                # Carriers table schema
â”‚       â””â”€â”€ warehouses_schema.json              # Warehouses table schema
â”‚
â”œâ”€â”€ ml/                                         # ML documentation
â”‚   â”œâ”€â”€ models/                                 # Model artifacts (MLflow managed)
â”‚   â”œâ”€â”€ feature_list.json                       # Feature catalog
â”‚   â”œâ”€â”€ model_card.md                           # Model documentation
â”‚   â””â”€â”€ mlflow_experiment_setup.md              # MLflow setup guide
â”‚
â”œâ”€â”€ config/                                     # Configuration files
â”‚   â”œâ”€â”€ environment.yml                         # Conda environment
â”‚   â”œâ”€â”€ automl_settings.json                    # FLAML AutoML config
â”‚   â””â”€â”€ fabric_lakehouse_paths.yaml             # Fabric resource IDs
â”‚
â”œâ”€â”€ scripts/                                    # Setup and validation scripts
â”‚   â”œâ”€â”€ setup_fabric_environment.py             # Environment validation
â”‚   â””â”€â”€ validate_semantic_model.py              # Schema validation
â”‚
â””â”€â”€ README.md                                   # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Microsoft Fabric workspace with:
  - Lakehouse with shipments, carriers, warehouses tables
  - Power BI semantic model: `delivery semantic model`
  - MLflow experiment and model registry enabled
- Python 3.10+ (automatically available in Fabric notebooks)

### Setup Steps

#### 1. **Upload Notebooks to Fabric**

1. Navigate to your Fabric workspace (Data Engineering or Data Science experience)
2. Upload the three notebooks:
   - `01_semantic_link_data_preparation.ipynb`
   - `02_autoML_training_pipeline.ipynb`
   - `03_batch_scoring_pipeline.ipynb`
3. Attach notebooks to your default Lakehouse

#### 2. **Configure Fabric Resources**

Update `config/fabric_lakehouse_paths.yaml` with your Fabric workspace details:

```yaml
workspace_id: "your-workspace-id"
lakehouse_id: "your-lakehouse-id"
semantic_model_name: "delivery semantic model"
mlflow_experiment_name: "delivery-time-prediction"
model_registry_name: "POC-DeliveryTimeModel-AutoML-Safe"
```

#### 3. **Validate Semantic Model**

Run the validation script to ensure your semantic model has the required tables:

```python
# In a Fabric notebook cell
%run scripts/validate_semantic_model.py
```

This checks for:
- `shipments` table with required columns
- `carriers` table with required columns
- `warehouses` table with required columns

#### 4. **Run the ML Pipeline**

Execute notebooks **in order**:

1. **Data Preparation** - `01_semantic_link_data_preparation.ipynb`
2. **Model Training** - `02_autoML_training_pipeline.ipynb` (~3 minutes)
3. **Batch Scoring** - `03_batch_scoring_pipeline.ipynb`

#### 5. **Connect Power BI Report**

1. Open `powerbi/delivery semantic model.pbix`
2. Verify semantic model connection
3. Predictions appear automatically via Direct Lake mode
4. Publish to Power BI Service

---

## ğŸ”„ Retraining the Model

To retrain with new data:

1. Refresh your Power BI semantic model
2. Run `02_autoML_training_pipeline.ipynb`
3. Run `03_batch_scoring_pipeline.ipynb` for updated predictions

**When to retrain:**
- Monthly (recommended)
- When adding new carriers or warehouses
- If prediction accuracy drops

---

## ğŸ“Š Power BI Dashboards

### Recommended Visuals

**Page 1 - Executive Overview**
- KPI cards: MAE, Avg Predicted Days, Avg Actual Days
- Line chart: Actual vs Predicted over time
- Error distribution histogram

**Page 2 - Carrier Performance**
- MAE by Carrier (bar chart)
- Prediction Bias by Carrier
- Shipment Count by Carrier

**Page 3 - Shipment Explorer**
- Detailed shipment table
- Filters: Carrier, Warehouse, Region, Service Level
- Prediction error scatter plot

### Key DAX Measures

```dax
AvgActualDeliveryDays = AVERAGE(shipments[delivery_days])
AvgPredictedDeliveryDays = AVERAGE(shipment_predictions[predicted_delivery_days])
```

See `powerbi/dax/` folder for complete measures.

---

## ğŸ“š What's Included

### Notebooks
- **Data Preparation** - Load from semantic model, validate, engineer features
- **Training Pipeline** - AutoML training with MLflow tracking
- **Batch Scoring** - Generate predictions for all shipments

### Utilities (`notebooks/utils/`)
- **preprocessing.py** - Data validation and cleaning
- **feature_engineering.py** - Feature creation functions
- **model_utils.py** - Model evaluation and MLflow helpers

### Configuration (`config/`)
- **automl_settings.json** - AutoML configuration (180 sec, MAE metric)
- **environment.yml** - Python environment setup
- **fabric_lakehouse_paths.yaml** - Fabric resource IDs

### Documentation (`ml/`)
- **model_card.md** - Model documentation
- **feature_list.json** - Feature catalog (9 features)
- **mlflow_experiment_setup.md** - MLflow setup guide

### Data Schemas (`data/schema/`)
- JSON schemas for shipments, carriers, warehouses tables

---

## ğŸ“ˆ Model Performance

Expected performance:
- **MAE:** ~1.2 days (average error)
- **RMSE:** ~1.8 days
- **RÂ² Score:** ~0.85

*Performance varies based on your data*

---

## ğŸ” Troubleshooting

**Semantic Link connection fails**
- Verify semantic model name and workspace permissions

**MLflow model not found**
- Check model registry name in MLflow experiments

**Power BI shows no predictions**
- Verify `shipment_predictions` table exists in Lakehouse

---

## ğŸ“š Resources

- [Semantic Link Documentation](https://learn.microsoft.com/fabric/data-science/semantic-link-overview)
- [FLAML AutoML](https://microsoft.github.io/FLAML/)
- [MLflow in Fabric](https://learn.microsoft.com/fabric/data-science/mlflow-overview)

---

## ğŸ¯ Next Steps

Ideas for extending this POC:

- Add weather data or traffic patterns
- Deploy as real-time API endpoint
- Add SHAP values for prediction explanations
- Schedule automated retraining
- A/B test model versions

---

## ğŸ“„ License

This POC is provided as-is for demonstration purposes.

---

**Built with Microsoft Fabric, Semantic Link, FLAML AutoML, and Power BI** ğŸš€
